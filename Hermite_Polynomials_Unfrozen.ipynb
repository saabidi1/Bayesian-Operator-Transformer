{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9aplxxZQR6w",
        "outputId": "1dce9f5a-7b2d-459d-ca2e-dece07083d86"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Joint training of MLP+Transformer on all tasks ---\n",
            "Epoch 1000  avg loss = 855.5754\n",
            "Epoch 2000  avg loss = 719.9027\n",
            "Epoch 3000  avg loss = 573.1282\n",
            "Epoch 4000  avg loss = 399.9585\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sympy import symbols, lambdify\n",
        "from sympy import symbols, hermite\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 1) DATA‐GEN HELPERS\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "x_sym = symbols('x')\n",
        "\n",
        "def sample_hermite_function(p, q, coeff_variance=1.0):\n",
        "    \"\"\"\n",
        "    Returns a NumPy‐vectorized function f(x) = sum_{i=p}^q c_i H_i(x),\n",
        "    with c_i ~ N(0, coeff_variance).\n",
        "    \"\"\"\n",
        "    expr = 0\n",
        "    for i in range(p, q+1):\n",
        "        c_i = np.random.normal(scale=np.sqrt(coeff_variance))\n",
        "        expr += c_i * hermite(i, x_sym)\n",
        "    return lambdify(x_sym, expr, 'numpy')\n",
        "\n",
        "def generate_dataset(num_tasks, p, q, seq_len,\n",
        "                     coeff_variance=1.0,\n",
        "                     x_variance=1.0):\n",
        "    \"\"\"\n",
        "    Generates `num_tasks` tasks.  Each task is a (seq_len × 2) NumPy array:\n",
        "      - col0:  x ~ N(0, x_variance)\n",
        "      - col1:  y = f(x) for a fresh random Hermite f of orders [p..q]\n",
        "    \"\"\"\n",
        "    tasks = []\n",
        "    for _ in range(num_tasks):\n",
        "        xs = np.random.normal(scale=np.sqrt(x_variance), size=seq_len)\n",
        "        f  = sample_hermite_function(p, q, coeff_variance)\n",
        "        ys = f(xs)\n",
        "        tasks.append(np.column_stack((xs, ys)))\n",
        "    return tasks\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 2) THE MODEL\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "class TinyTransformer(nn.Module):\n",
        "    def __init__(self, d_model=16, nhead=4):\n",
        "        super().__init__()\n",
        "        # (a) embed each 2-vector [x_t,y_t] → d_model\n",
        "        self.embedding_mlp = nn.Sequential(\n",
        "            nn.Linear(2, d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model),\n",
        "        )\n",
        "        # (b) one layer of causal self-attention\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            batch_first=True,   # we have [B, S, d_model]\n",
        "        )\n",
        "        # create a mask to prevent attending to future positions:\n",
        "        # mask shape [S, S], True means masked\n",
        "        self.register_buffer(\"mask\", None)  # will build on the fly\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
        "        # (c) read-out final token → scalar\n",
        "        self.readout = nn.Linear(d_model, 1)\n",
        "\n",
        "    def forward(self, x_seq):\n",
        "        \"\"\"\n",
        "        x_seq: [B, S, 2] tensor of (x_t,y_t) pairs\n",
        "        returns y_pred: [B] of final predictions\n",
        "        \"\"\"\n",
        "        B, S, F = x_seq.shape       # F==2\n",
        "\n",
        "        # 1) embed every time‐step\n",
        "        flat     = x_seq.view(B*S, F)              # → [B*S, 2]\n",
        "        emb_flat = self.embedding_mlp(flat)        # → [B*S, d_model]\n",
        "        emb      = emb_flat.view(B, S, -1)         # → [B, S, d_model]\n",
        "\n",
        "        # 2) build causal mask once if needed\n",
        "        if self.mask is None or self.mask.size(0) != S:\n",
        "            # square mask: True in upper triangle above diag\n",
        "            self.mask = torch.triu(torch.ones(S,S,dtype=torch.bool), diagonal=1).to(emb.device)\n",
        "\n",
        "        # 3) apply masked Transformer\n",
        "        attn_out = self.transformer(emb, mask=self.mask)  # → [B, S, d_model]\n",
        "\n",
        "        # 4) grab last time‐step\n",
        "        last = attn_out[:, -1, :]               # → [B, d_model]\n",
        "\n",
        "        # 5) project to a scalar\n",
        "        y_pred = self.readout(last).squeeze(-1) # → [B]\n",
        "        return y_pred\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 3) DRIVER / TRAINING + VALIDATION\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(0)\n",
        "    np.random.seed(0)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # 3.1) HYPERPARAMETERS\n",
        "    p, q   = 0, 3    # Hermite orders in [p..q]\n",
        "    N1, T1 = 500,  8  # first group: 50 tasks, each length 8\n",
        "    N2, T2 = 800, 12  # second group: 30 tasks, each length 12\n",
        "\n",
        "    # 3.2) GENERATE + CORRUPT DATA\n",
        "    training_data1 = generate_dataset(T1, p, q, N1, coeff_variance=2.0, x_variance=1.0)\n",
        "    training_data2 = generate_dataset(T2, p, q, N2, coeff_variance=2.0, x_variance=1.0)\n",
        "\n",
        "    # extract true last‐y and zero it in place\n",
        "    y_last1 = []\n",
        "    for task in training_data1:\n",
        "        y_last1.append(float(task[-1,1]))\n",
        "        task[-1,1] = 0.0\n",
        "\n",
        "    y_last2 = []\n",
        "    for task in training_data2:\n",
        "        y_last2.append(float(task[-1,1]))\n",
        "        task[-1,1] = 0.0\n",
        "\n",
        "    # combine both groups into one training list\n",
        "    train_tasks = training_data1 + training_data2\n",
        "    train_y     = y_last1 + y_last2\n",
        "    total_tasks = len(train_tasks)  # = N1 + N2\n",
        "\n",
        "    # 3.3) INSTANTIATE MODEL, LOSS, OPTIMIZER\n",
        "    model     = TinyTransformer(d_model=5, nhead=1).to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    # 3.4) TRAINING LOOP (no freezing!)\n",
        "    print(\"\\n--- Joint training of MLP+Transformer on all tasks ---\")\n",
        "    for epoch in range(1, 10000):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        # shuffle tasks each epoch\n",
        "        idxs = np.random.permutation(total_tasks)\n",
        "        for i in idxs:\n",
        "            task = train_tasks[i]         # NumPy array (T,2)\n",
        "            y_true = train_y[i]           # float scalar\n",
        "\n",
        "            # build input [1, T, 2]\n",
        "            x_seq = torch.from_numpy(task[:,:2])\\\n",
        "                          .unsqueeze(0)\\\n",
        "                          .float()\\\n",
        "                          .to(device)      # → [1, T, 2]\n",
        "            y_true = torch.tensor([y_true], dtype=torch.float32, device=device)  # [1]\n",
        "\n",
        "            # forward + backward\n",
        "            pred = model(x_seq)           # → [1]\n",
        "            loss = criterion(pred, y_true)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        if epoch % 1000 == 0:\n",
        "            avg = total_loss / total_tasks\n",
        "            print(f\"Epoch {epoch:4d}  avg loss = {avg:.4f}\")\n",
        "\n",
        "    # 3.5) VALIDATION\n",
        "    model.eval()\n",
        "    T_val  = 100\n",
        "    N_star = 15\n",
        "    val_tasks = generate_dataset(T_val, p, q, N_star, coeff_variance=2.0, x_variance=1.0)\n",
        "\n",
        "    val_y_true = []\n",
        "    for task in val_tasks:\n",
        "        val_y_true.append(float(task[-1,1]))\n",
        "        task[-1,1] = 0.0\n",
        "\n",
        "    val_preds = []\n",
        "    with torch.no_grad():\n",
        "        for task in val_tasks:\n",
        "            x_seq = torch.from_numpy(task[:,:2])\\\n",
        "                          .unsqueeze(0)\\\n",
        "                          .float()\\\n",
        "                          .to(device)    # [1, N_star, 2]\n",
        "            pred = model(x_seq).item()\n",
        "            val_preds.append(pred)\n",
        "\n",
        "    val_preds = np.array(val_preds)\n",
        "    val_trues = np.array(val_y_true)\n",
        "    val_mse   = np.mean((val_preds - val_trues)**2)\n",
        "\n",
        "    print(f\"\\nValidation on {T_val} tasks of length {N_star}\")\n",
        "    print(f\"  Avg MSE = {val_mse:.4f}\")\n",
        "    for i,(t,p) in enumerate(zip(val_trues, val_preds),1):\n",
        "        print(f\" Task {i:2d}: true={t:.4f}  pred={p:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}