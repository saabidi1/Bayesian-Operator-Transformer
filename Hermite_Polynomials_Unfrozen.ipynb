{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9aplxxZQR6w",
        "outputId": "1dce9f5a-7b2d-459d-ca2e-dece07083d86"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "from sympy import symbols, lambdify\n",
        "from sympy import symbols, hermite\n",
        "from torch.distributions import Normal\n",
        "from numpy.polynomial.hermite import Hermite, herm2poly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ------------------------\n",
        "# Snippet 0: Device Setup\n",
        "# ------------------------\n",
        "# Use Apple M1’s GPU via MPS if available, else CPU\n",
        "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
        "print(f\"Using device: {device}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# 1) DATA‐GEN HELPERS\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "x_sym = symbols('x')\n",
        "\n",
        "def sample_hermite_function(p, q, coeff_variance=1.0):\n",
        "    \"\"\"\n",
        "    Returns a NumPy‐vectorized function f(x) = sum_{i=p}^q c_i H_i(x),\n",
        "    with c_i ~ N(0, coeff_variance).\n",
        "    \"\"\"\n",
        "    expr = 0\n",
        "    for i in range(p, q+1):\n",
        "        c_i = np.random.normal(scale=np.sqrt(coeff_variance))\n",
        "        expr += c_i * hermite(i, x_sym)\n",
        "    return lambdify(x_sym, expr, 'numpy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BayesianTransformerRegressor(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim=2,\n",
        "        d_model=100,\n",
        "        nhead=1,\n",
        "        dim_feedforward=100,\n",
        "        num_encoder_layers=1,\n",
        "        p_dropout=0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # ————————————\n",
        "        # 1) Embed (x,y) → d_model + nonlinearity\n",
        "        # ————————————\n",
        "        self.embed = nn.Sequential(\n",
        "            nn.Linear(input_dim, d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p_dropout),\n",
        "\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.Dropout(p_dropout)\n",
        "\n",
        "        )\n",
        "\n",
        "        # —————————————————————\n",
        "        # 2) Transformer encoder stack\n",
        "        # —————————————————————\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=p_dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(enc_layer, num_encoder_layers)\n",
        "\n",
        "        # ———————————————————————\n",
        "        # 3) Deep output head (2 extra layers)\n",
        "        # ———————————————————————\n",
        "        self.out_mlp = nn.Sequential(\n",
        "\n",
        "            nn.Linear(d_model, 2),\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, seq):\n",
        "        \"\"\"\n",
        "        seq: (batch, seq_len, 2) tensor of (x, masked_y)\n",
        "        returns: (mean, std) each of shape (batch, seq_len)\n",
        "        \"\"\"\n",
        "        # 1) embed + nonlinearity\n",
        "        h = self.embed(seq)                # → (B, L, d_model)\n",
        "\n",
        "        # 2) build causal mask so j > i positions are masked out\n",
        "        L = h.size(1)\n",
        "        mask = torch.triu(torch.full((L, L), float('-inf'), device=h.device), diagonal=1)\n",
        "\n",
        "        # 3) transformer with causal attention\n",
        "        h = self.encoder(h, mask=mask)     # → (B, L, d_model)\n",
        "\n",
        "        # 4) deep MLP head to [mean, log_std]\n",
        "        out = self.out_mlp(h)              # → (B, L, 2)\n",
        "        mean    = out[..., 0]              # → (B, L)\n",
        "        log_std = out[..., 1]\n",
        "        std     = torch.exp(log_std)       # ensure positivity\n",
        "\n",
        "        return mean, std\n",
        "\n",
        "# instantiate and move to GPU/CPU\n",
        "model = BayesianTransformerRegressor(\n",
        "    input_dim=2,\n",
        "    d_model=64,             # you can bump this up too\n",
        "    nhead=1,\n",
        "    dim_feedforward=64,\n",
        "    num_encoder_layers=1,\n",
        "    p_dropout=0.01\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 100/10000 — NLL: 3.4025\n",
            "Mean: -0.1479 ± 6.2098\n",
            "True last y: -0.3638 ± 8.4985\n",
            "Training on N=3...\n",
            "\n",
            "Training on function 1...\n",
            "\n",
            "Epoch 200/10000 — NLL: 3.1446\n",
            "Mean: -0.1836 ± 5.7083\n",
            "True last y: -0.3638 ± 8.4985\n",
            "Training on N=3...\n",
            "\n",
            "Training on function 1...\n",
            "\n",
            "Epoch 300/10000 — NLL: 3.0208\n",
            "Mean: -0.1003 ± 5.3772\n",
            "True last y: -0.3638 ± 8.4985\n",
            "Training on N=3...\n",
            "\n",
            "Training on function 1...\n",
            "\n",
            "Epoch 400/10000 — NLL: 2.9447\n",
            "Mean: 0.2459 ± 5.1093\n",
            "True last y: -0.3638 ± 8.4985\n",
            "Training on N=3...\n",
            "\n",
            "Training on function 1...\n",
            "\n",
            "Epoch 500/10000 — NLL: 2.7635\n",
            "Mean: 0.8665 ± 4.7276\n",
            "True last y: -0.3638 ± 8.4985\n",
            "Training on N=3...\n",
            "\n",
            "Training on function 1...\n",
            "\n",
            "Epoch 600/10000 — NLL: 2.4798\n",
            "Mean: 1.6024 ± 3.9324\n",
            "True last y: -0.3638 ± 8.4985\n",
            "Training on N=3...\n",
            "\n",
            "Training on function 1...\n",
            "\n",
            "Epoch 700/10000 — NLL: 1.8204\n",
            "Mean: 2.1859 ± 2.7759\n",
            "True last y: -0.3638 ± 8.4985\n",
            "Training on N=3...\n",
            "\n",
            "Training on function 1...\n",
            "\n",
            "Epoch 800/10000 — NLL: 1.3155\n",
            "Mean: 2.5177 ± 3.0084\n",
            "True last y: -0.3638 ± 8.4985\n",
            "Training on N=3...\n",
            "\n",
            "Training on function 1...\n",
            "\n",
            "Epoch 900/10000 — NLL: 1.0479\n",
            "Mean: 2.6554 ± 3.3203\n",
            "True last y: -0.3638 ± 8.4985\n",
            "Training on N=3...\n",
            "\n",
            "Training on function 1...\n",
            "\n",
            "Epoch 1000/10000 — NLL: 0.9896\n",
            "Mean: 2.6145 ± 3.0796\n",
            "True last y: -0.3638 ± 8.4985\n",
            "Training on N=3...\n",
            "\n",
            "Training on function 1...\n",
            "\n",
            "Epoch 1100/10000 — NLL: 1.0216\n",
            "Mean: 2.5759 ± 3.4226\n",
            "True last y: -0.3638 ± 8.4985\n",
            "Training on N=3...\n",
            "\n",
            "Training on function 1...\n",
            "\n",
            "Epoch 1200/10000 — NLL: 0.9174\n",
            "Mean: 2.4250 ± 2.9866\n",
            "True last y: -0.3638 ± 8.4985\n",
            "Training on N=3...\n",
            "\n",
            "Training on function 1...\n",
            "\n",
            "Epoch 1300/10000 — NLL: 0.7933\n",
            "Mean: 2.2906 ± 2.8330\n",
            "True last y: -0.3638 ± 8.4985\n",
            "Training on N=3...\n",
            "\n",
            "Training on function 1...\n",
            "\n",
            "Epoch 1400/10000 — NLL: 0.8234\n",
            "Mean: 2.1577 ± 2.3258\n",
            "True last y: -0.3638 ± 8.4985\n",
            "Training on N=3...\n",
            "\n",
            "Training on function 1...\n",
            "\n",
            "Epoch 1500/10000 — NLL: 1.0310\n",
            "Mean: 2.1243 ± 2.4680\n",
            "True last y: -0.3638 ± 8.4985\n",
            "Training on N=3...\n",
            "\n",
            "Training on function 1...\n",
            "\n",
            "Epoch 1600/10000 — NLL: 0.4913\n",
            "Mean: 1.9375 ± 2.1990\n",
            "True last y: -0.3638 ± 8.4985\n",
            "Training on N=3...\n",
            "\n",
            "Training on function 1...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 1: Generate M training tasks, each length N, mask last y\n",
        "\n",
        "#Give N_range\n",
        "N_range = np.linspace(3,15,5, dtype=int)\n",
        "p, q = 0, 3 \n",
        "num_function = 10\n",
        "# Generate M tasks of length N\n",
        "for N in N_range:\n",
        "\n",
        "    for i in range(1,num_function):\n",
        "        M = 50\n",
        "        tasks_x = np.random.normal(0, 1, size=(M, N))\n",
        "        tasks_x.sort(axis=1)\n",
        "        f = sample_hermite_function(p, q, coeff_variance = 1)\n",
        "        tasks_y = f(tasks_x)\n",
        "        tasks_y_masked = tasks_y.copy()\n",
        "        tasks_y_masked[:, -1] = 0.0\n",
        "        data = np.stack([tasks_x, tasks_y_masked], axis=2)  # (M, N, 2)\n",
        "\n",
        "        # Move to torch tensors\n",
        "        data_t    = torch.tensor(data, dtype=torch.float32).to(device)      # (M, N, 2)\n",
        "        true_last = torch.tensor(tasks_y[:, -1], dtype=torch.float32).to(device)  # (M,)\n",
        "\n",
        "\n",
        "\n",
        "        # 4️⃣ Training loop with negative log-likelihood\n",
        "        # ---------------------------------------\n",
        "        # Snippet 3: Training Loop with MC-Dropout\n",
        "        # ---------------------------------------\n",
        "        # Move data to device\n",
        "\n",
        "        optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "        epochs    = 10000\n",
        "\n",
        "        for ep in range(1, epochs+1):\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            mean_all, std_all = model(data_t)       # (M, N), (M, N)\n",
        "            mean_last = mean_all[:, -1]             # (M,)\n",
        "            std_last  = std_all[:, -1]              # (M,)\n",
        "\n",
        "            dist = Normal(mean_last, std_last)\n",
        "            log_p = dist.log_prob(true_last)        # (M,)\n",
        "            loss  = -log_p.mean()                   # negative log-likelihood\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if ep % 100 == 0:\n",
        "                print(f\"Epoch {ep}/{epochs} — NLL: {loss.item():.4f}\")\n",
        "                print(f\"Mean: {mean_last.mean():.4f} ± {std_last.mean():.4f}\")\n",
        "                print(f\"True last y: {true_last.mean():.4f} ± {true_last.std():.4f}\")\n",
        "                #Print which N_range value is being trained\n",
        "                print(f\"Training on N={N}...\\n\")\n",
        "                print(f\"Training on function {i}...\\n\")\n",
        "\n",
        "        print('Training Complete')\n",
        "\n",
        "        # 1e) Visualize task #0 with scatter\n",
        "        plt.figure(figsize=(6,4))\n",
        "        # Plot all masked training points as circles\n",
        "        plt.scatter(\n",
        "            data[0, :, 0],        # x-coordinates\n",
        "            data[0, :, 1],        # masked y-values\n",
        "            marker='o',\n",
        "            color='tab:blue',\n",
        "            label='masked y'\n",
        "        )\n",
        "        # Highlight the true last yₙ as a larger square\n",
        "        plt.scatter(\n",
        "            tasks_x[0, -1],       # xₙ\n",
        "            tasks_y[0, -1],       # true yₙ\n",
        "            marker='s',\n",
        "            s=100,\n",
        "            color='tab:red',\n",
        "            label='true yₙ'\n",
        "        )\n",
        "        plt.title(\"Example Task (idx=0): Training Sequence with Masked Last Value\")\n",
        "        plt.xlabel(\"x\")\n",
        "        plt.ylabel(\"y\")\n",
        "        plt.legend()\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(1, num_function):\n",
        "    # 5️⃣ MC-Dropout Inference on test_num random test tasks\n",
        "    test_num, T = 100, 100\n",
        "    f = sample_hermite_function(p, q, coeff_variance=1.0)\n",
        "    # Create new random tasks\n",
        "    tx = np.random.normal(-1, 1, size=(test_num, 10))\n",
        "    tx.sort(axis=1)\n",
        "    ty = f(tx)\n",
        "    ty_mask = ty.copy()\n",
        "    ty_mask[:, -1] = 0.0\n",
        "    data_rand   = np.stack([tx, ty_mask], axis=2)\n",
        "    data_rand_t = torch.tensor(data_rand, dtype=torch.float32).to(device)\n",
        "\n",
        "    model.train()  # keep dropout active\n",
        "    all_samps = []\n",
        "\n",
        "    for _ in range(T):\n",
        "        with torch.no_grad():\n",
        "            m_r, s_r = model(data_rand_t)\n",
        "            # sample y_n ~ Normal(mean, std)\n",
        "            samp = m_r[:, -1] + s_r[:, -1] * torch.randn(test_num, device=device)\n",
        "            all_samps.append(samp.cpu().numpy())\n",
        "\n",
        "    all_samps = np.stack(all_samps, axis=0)      # (T, test_num)\n",
        "    mean_pred = all_samps.mean(axis=0)           # (test_num,)\n",
        "    std_pred  = all_samps.std(axis=0)\n",
        "\n",
        "    # Plot predictions ±2σ vs. true y_n\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.errorbar(\n",
        "        tx[:, -1], mean_pred,\n",
        "        yerr=2*std_pred,\n",
        "        fmt='o', ecolor='lightgray', capsize=3,\n",
        "        label='Predicted ±2σ'\n",
        "    )\n",
        "    plt.scatter(\n",
        "        tx[:, -1], ty[:, -1],\n",
        "        color='red', marker='x', s=50,\n",
        "        label='True yₙ'\n",
        "    )\n",
        "    plt.title(\"MC-Dropout on 100 Random Test Tasks\")\n",
        "    plt.xlabel(\"xₙ\")\n",
        "    plt.ylabel(\"yₙ\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
